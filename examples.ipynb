{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Virtual Environment and Install Requirements\n",
    "#!python3 -m venv ../brain_model_env\n",
    "#!source ../brain_model_env/bin/activate\n",
    "#!python3 -m ipykernel install --user --name=brain_model_env --display-name \"Python (brain_model_env)\"\n",
    "#remember to switch to notebook/virtual environment kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_model.model_files.sfcn import SFCN\n",
    "from dp_model import dp_loss as dpl\n",
    "from dp_model import dp_utils as dpu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sex Prediction Test on ABIDEI (0 F 1 M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:33<00:00,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished inference on 985 subjects\n",
      "Accuracy: 84.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "input_root = '../ABIDE_Dataset/data/Cropped/ABIDEI'  # Root folder with sub-xxxx/anat/*.nii.gz\n",
    "participants_path = '../ABIDE_Dataset/data/ABIDEI/participants.tsv'\n",
    "model_weights_path = './sex_prediction/run_20191008_00_epoch_last.p'\n",
    "label_column = 'sex'  # or whatever your ground truth column is\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN(output_dim=2, channel_number=[28, 58, 128, 256, 256, 64])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load labels\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference loop\n",
    "# -----------------------\n",
    "records = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through subject subfolders\n",
    "for subject_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    # Full path to T1w file\n",
    "    anat_dir = os.path.join(input_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    # Find the T1w NIfTI file (assuming one per subject)\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii.gz')]\n",
    "    t1w_file = None\n",
    "    for f in nii_files:\n",
    "        if subject_id in f and 'T1w' in f:\n",
    "            t1w_file = f\n",
    "            break\n",
    "\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(anat_dir, t1w_file)\n",
    "\n",
    "    if subject_id not in df.index:\n",
    "        print(f\"{subject_id} not in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    true_label = df.loc[subject_id, label_column]\n",
    "\n",
    "    # Load and normalize\n",
    "    data = nib.load(full_path).get_fdata()\n",
    "    data = data / data.mean()\n",
    "\n",
    "    # Prepare input tensor (1, 1, D, H, W)\n",
    "    input_tensor = torch.tensor(data.reshape((1,) + data.shape), dtype=torch.float32).cuda()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = output[0].cpu().reshape([1, -1])\n",
    "        probs = np.exp(probs.numpy().reshape(-1))\n",
    "\n",
    "        \n",
    "    pred_label = np.argmax(probs)\n",
    "    is_correct = int(pred_label == true_label)\n",
    "\n",
    "    correct += is_correct\n",
    "    total += 1\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'prob_female': probs[0],\n",
    "        'prob_male': probs[1],\n",
    "        'correct': is_correct\n",
    "    })\n",
    "\n",
    "# -----------------------\n",
    "# Save and report\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv('../ABIDE_Dataset/outputs/ABIDEI/sfcn_sex_predictions.csv', index=False)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nFinished inference on {total} subjects\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sex Prediction Test on ABIDEII (0 F 1 M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 346/960 [00:48<00:08, 68.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-29057 not in participants.tsv\n",
      "sub-29058 not in participants.tsv\n",
      "sub-29059 not in participants.tsv\n",
      "sub-29060 not in participants.tsv\n",
      "sub-29062 not in participants.tsv\n",
      "sub-29063 not in participants.tsv\n",
      "sub-29064 not in participants.tsv\n",
      "sub-29065 not in participants.tsv\n",
      "sub-29066 not in participants.tsv\n",
      "sub-29067 not in participants.tsv\n",
      "sub-29068 not in participants.tsv\n",
      "sub-29069 not in participants.tsv\n",
      "sub-29070 not in participants.tsv\n",
      "sub-29071 not in participants.tsv\n",
      "sub-29072 not in participants.tsv\n",
      "sub-29073 not in participants.tsv\n",
      "sub-29074 not in participants.tsv\n",
      "sub-29075 not in participants.tsv\n",
      "sub-29076 not in participants.tsv\n",
      "sub-29077 not in participants.tsv\n",
      "sub-29078 not in participants.tsv\n",
      "sub-29079 not in participants.tsv\n",
      "sub-29080 not in participants.tsv\n",
      "sub-29081 not in participants.tsv\n",
      "sub-29082 not in participants.tsv\n",
      "sub-29083 not in participants.tsv\n",
      "sub-29085 not in participants.tsv\n",
      "sub-29086 not in participants.tsv\n",
      "sub-29087 not in participants.tsv\n",
      "sub-29088 not in participants.tsv\n",
      "sub-29089 not in participants.tsv\n",
      "sub-29090 not in participants.tsv\n",
      "sub-29091 not in participants.tsv\n",
      "sub-29092 not in participants.tsv\n",
      "sub-29093 not in participants.tsv\n",
      "sub-29094 not in participants.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [02:24<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished inference on 924 subjects\n",
      "Accuracy: 75.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "input_root = '../ABIDE_Dataset/data/Cropped/ABIDEII'  # Root folder with sub-xxxx/anat/*.nii.gz\n",
    "participants_path = '../ABIDE_Dataset/data/ABIDEII/participants.tsv'\n",
    "model_weights_path = './sex_prediction/run_20191008_00_epoch_last.p'\n",
    "label_column = 'sex'  # or whatever your ground truth column is\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN(output_dim=2, channel_number=[28, 58, 128, 256, 256, 64])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load labels\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference loop\n",
    "# -----------------------\n",
    "records = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through subject subfolders\n",
    "for subject_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    # Full path to T1w file\n",
    "    anat_dir = os.path.join(input_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    # Find the T1w NIfTI file (assuming one per subject)\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii.gz')]\n",
    "    t1w_file = None\n",
    "    for f in nii_files:\n",
    "        if subject_id in f and 'T1w' in f:\n",
    "            t1w_file = f\n",
    "            break\n",
    "\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(anat_dir, t1w_file)\n",
    "\n",
    "    if subject_id not in df.index:\n",
    "        print(f\"{subject_id} not in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    true_label = df.loc[subject_id, label_column]\n",
    "\n",
    "    # Load and normalize\n",
    "    data = nib.load(full_path).get_fdata()\n",
    "    data = data / data.mean()\n",
    "\n",
    "    # Prepare input tensor (1, 1, D, H, W)\n",
    "    input_tensor = torch.tensor(data.reshape((1,) + data.shape), dtype=torch.float32).cuda()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = output[0].cpu().reshape([1, -1])\n",
    "        probs = np.exp(probs.numpy().reshape(-1))\n",
    "\n",
    "        \n",
    "    pred_label = np.argmax(probs)\n",
    "    is_correct = int(pred_label == true_label)\n",
    "\n",
    "    correct += is_correct\n",
    "    total += 1\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'prob_female': probs[0],\n",
    "        'prob_male': probs[1],\n",
    "        'correct': is_correct\n",
    "    })\n",
    "\n",
    "# -----------------------\n",
    "# Save and report\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv('../ABIDE_Dataset/outputs/ABIDEII/sfcn_sex_predictions.csv', index=False)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nFinished inference on {total} subjects\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just predicts 1 for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sex Prediction Test on ABIDEI (0 M 1 F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:33<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished inference on 985 subjects\n",
      "Accuracy: 15.84%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "input_root = '../ABIDE_Dataset/data/Cropped/ABIDEI'  # Root folder with sub-xxxx/anat/*.nii.gz\n",
    "participants_path = '../ABIDE_Dataset/data/ABIDEI/participants_sex_switch.tsv'\n",
    "model_weights_path = './sex_prediction/run_20191008_00_epoch_last.p'\n",
    "label_column = 'sex'  # or whatever your ground truth column is\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN(output_dim=2, channel_number=[28, 58, 128, 256, 256, 64])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load labels\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference loop\n",
    "# -----------------------\n",
    "records = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through subject subfolders\n",
    "for subject_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    # Full path to T1w file\n",
    "    anat_dir = os.path.join(input_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    # Find the T1w NIfTI file (assuming one per subject)\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii.gz')]\n",
    "    t1w_file = None\n",
    "    for f in nii_files:\n",
    "        if subject_id in f and 'T1w' in f:\n",
    "            t1w_file = f\n",
    "            break\n",
    "\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(anat_dir, t1w_file)\n",
    "\n",
    "    if subject_id not in df.index:\n",
    "        print(f\"{subject_id} not in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    true_label = df.loc[subject_id, label_column]\n",
    "\n",
    "    # Load and normalize\n",
    "    data = nib.load(full_path).get_fdata()\n",
    "    data = data / data.mean()\n",
    "\n",
    "    # Prepare input tensor (1, 1, D, H, W)\n",
    "    input_tensor = torch.tensor(data.reshape((1,) + data.shape), dtype=torch.float32).cuda()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = output[0].cpu().reshape([1, -1])\n",
    "        probs = np.exp(probs.numpy().reshape(-1))\n",
    "\n",
    "        \n",
    "    pred_label = np.argmax(probs)\n",
    "    is_correct = int(pred_label == true_label)\n",
    "\n",
    "    correct += is_correct\n",
    "    total += 1\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'prob_female': probs[0],\n",
    "        'prob_male': probs[1],\n",
    "        'correct': is_correct\n",
    "    })\n",
    "\n",
    "# -----------------------\n",
    "# Save and report\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv('../ABIDE_Dataset/outputs/ABIDEI/sfcn_sex_1F_predictions.csv', index=False)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nFinished inference on {total} subjects\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still just predicts 1 for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:24<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete for 985 subjects\n",
      "Mean KL divergence loss: 93.1585\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Settings\n",
    "# ----------------------------\n",
    "image_root = \"../ABIDE_Dataset/data/Cropped/ABIDEI\"\n",
    "participants_path = \"../ABIDE_Dataset/data/ABIDEI/participants.tsv\"\n",
    "model_weights_path = \"./brain_age/run_20190719_00_epoch_best_mae.p\"\n",
    "output_csv_path = \"../ABIDE_Dataset/outputs/ABIDEI/sfcn_age_top1_predictions.csv\"\n",
    "\n",
    "bin_range = [0, 40]\n",
    "bin_step = 1\n",
    "sigma = 1\n",
    "input_shape = (160, 192, 160)\n",
    "\n",
    "# Compute bin centers\n",
    "bin_centers = np.arange(bin_range[0], bin_range[1], bin_step)  # [0, 1, ..., 39]\n",
    "num_bins = len(bin_centers)\n",
    "\n",
    "# ----------------------------\n",
    "# Load model\n",
    "# ----------------------------\n",
    "model = SFCN(output_dim=num_bins)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Load participants.tsv\n",
    "# ----------------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# ----------------------------\n",
    "# Inference Loop\n",
    "# ----------------------------\n",
    "records = []\n",
    "total_loss = 0\n",
    "total_subjects = 0\n",
    "\n",
    "for subject_id in tqdm(sorted(os.listdir(image_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    anat_dir = os.path.join(image_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "    t1w_file = next((f for f in nii_files if subject_id in f and 'T1w' in f), None)\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    if subject_id not in df.index or pd.isna(df.loc[subject_id, 'age']):\n",
    "        print(f\"{subject_id} missing age in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        data = nib.load(os.path.join(anat_dir, t1w_file)).get_fdata()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {subject_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    data = data / data.mean()\n",
    "\n",
    "    # Crop (or skip if image too small)\n",
    "    #if any(s < t for s, t in zip(data.shape, input_shape)):\n",
    "        #print(f\"{subject_id} image too small: {data.shape}\")\n",
    "        #continue\n",
    "\n",
    "    input_tensor = torch.tensor(data.reshape((1,) + data.shape), dtype=torch.float32).cuda()\n",
    "\n",
    "    # Soft label for KL loss\n",
    "    age = df.loc[subject_id, 'age']\n",
    "    y_soft, _ = dpu.num2vect([age], bin_range, bin_step, sigma)\n",
    "    y_soft = torch.tensor(y_soft, dtype=torch.float32)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # [1, num_bins]\n",
    "        logits = output[0].cpu()\n",
    "        probs = torch.softmax(logits, dim=0).numpy()\n",
    "\n",
    "        # Compute loss\n",
    "        kl_loss = dpl.my_KLDivLoss(logits.unsqueeze(0), y_soft).item()\n",
    "\n",
    "        # Top 2 predicted bins\n",
    "        top2_idx = np.argsort(probs)[-2:][::-1]\n",
    "        top2_probs = probs[top2_idx]\n",
    "        top2_bins = bin_centers[top2_idx]\n",
    "\n",
    "        records.append({\n",
    "            'subject_id': subject_id,\n",
    "            'true_age': age,\n",
    "            'top1_class': top2_bins[0],\n",
    "            'top1_prob': top2_probs[0],\n",
    "            'kl_div_loss': kl_loss\n",
    "        })\n",
    "\n",
    "        total_loss += kl_loss\n",
    "        total_subjects += 1\n",
    "\n",
    "# ----------------------------\n",
    "# Save and report\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "avg_loss = total_loss / total_subjects if total_subjects > 0 else float('nan')\n",
    "print(f\"\\nInference complete for {total_subjects} subjects\")\n",
    "print(f\"Mean KL divergence loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: torch.Size([1, 40])\n",
      "Input data shape: torch.Size([1, 1, 160, 192, 160])\n",
      "dtype: torch.float32\n",
      "Output shape: torch.Size([1, 40])\n",
      "1.7161434\n",
      "\n",
      "ðŸ“Œ Predicted most probable age bin: 17.5\n",
      "ðŸ“ˆ Probability: 4.31%\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# Note: the example model checkpoint works with UK Biobank T1_brain_linearto_MNI.nii.gz files\n",
    "model = SFCN()\n",
    "model = torch.nn.DataParallel(model)\n",
    "fp_ = './brain_age/run_20190719_00_epoch_best_mae.p'\n",
    "model.load_state_dict(torch.load(fp_, weights_only=True))\n",
    "model.cuda()\n",
    "\n",
    "# Example data: some random brain in the MNI152 1mm std space\n",
    "image_path = \"../ABIDE_Dataset/data/Cropped/ABIDEI/sub-0051291/anat/sub-0051291_T1w.nii.gz\"\n",
    "img = nib.load(image_path)\n",
    "data = np.squeeze(img.get_fdata())\n",
    "label = np.array([16.47,])\n",
    "\n",
    "# Transforming the age to soft label (probability distribution)\n",
    "bin_range = [0,40]\n",
    "bin_step = 1\n",
    "sigma = 1\n",
    "y, bc = dpu.num2vect(label, bin_range, bin_step, sigma)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "print(f'Label shape: {y.shape}')\n",
    "\n",
    "# Preprocessing\n",
    "data = data/data.mean()\n",
    "\n",
    "# Move the data from numpy to torch tensor on GPU\n",
    "sp = (1,1)+data.shape\n",
    "data = data.reshape(sp)\n",
    "input_data = torch.tensor(data, dtype=torch.float32).cuda()\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "print(f'dtype: {input_data.dtype}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval() # Don't forget this. BatchNorm will be affected if not in eval mode.\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "# Output, loss, visualisation\n",
    "x = output[0].cpu().reshape([1, -1])\n",
    "print(f'Output shape: {x.shape}')\n",
    "loss = dpl.my_KLDivLoss(x, y).numpy()\n",
    "print(loss)\n",
    "\n",
    "# Prediction and Summary\n",
    "x = x.numpy().reshape(-1)  # Model output (logits)\n",
    "y = y.numpy().reshape(-1)  # True soft label\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = np.exp(x)\n",
    "probs /= probs.sum()  # Ensure it's normalized\n",
    "\n",
    "# Get the index and value of the max probability\n",
    "top_idx = np.argmax(probs)\n",
    "top_bin = bc[top_idx]\n",
    "top_prob = probs[top_idx]\n",
    "\n",
    "print(f\"\\nðŸ“Œ Predicted most probable age bin: {top_bin}\")\n",
    "print(f\"ðŸ“ˆ Probability: {top_prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 985/985 [02:32<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to ../ABIDE_Dataset/outputs/ABIDEI/sfcn_age_predictions.csv\n",
      "\n",
      "ðŸ“Š Evaluated 985 subjects\n",
      "ðŸ”» Average KLDiv loss: 1.877063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "image_root = \"../ABIDE_Dataset/data/Cropped/ABIDEI\"\n",
    "participants_tsv = \"../ABIDE_Dataset/data/ABIDEI/participants.tsv\"\n",
    "model_weights = \"./brain_age/run_20190719_00_epoch_best_mae.p\"\n",
    "output_csv = \"../ABIDE_Dataset/outputs/ABIDEI/sfcn_age_predictions.csv\"\n",
    "\n",
    "bin_range = [0, 40]\n",
    "bin_step = 1\n",
    "sigma = 1\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN()\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights, weights_only=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load participants info\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_tsv, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference\n",
    "# -----------------------\n",
    "records = []\n",
    "total_loss = 0\n",
    "total_subjects = 0\n",
    "\n",
    "bin_centers = np.arange(bin_range[0], bin_range[1], bin_step)\n",
    "\n",
    "for subject_id in tqdm(sorted(os.listdir(image_root))):\n",
    "    if not subject_id.startswith(\"sub-\"):\n",
    "        continue\n",
    "\n",
    "    anat_path = os.path.join(image_root, subject_id, \"anat\")\n",
    "    if not os.path.isdir(anat_path):\n",
    "        continue\n",
    "\n",
    "    nii_files = [f for f in os.listdir(anat_path) if f.endswith(\".nii.gz\") and \"T1w\" in f and subject_id in f]\n",
    "    if len(nii_files) == 0:\n",
    "        print(f\"No T1w image found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(anat_path, nii_files[0])\n",
    "\n",
    "    if subject_id not in df.index:\n",
    "        print(f\"{subject_id} not in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    # Load and preprocess image\n",
    "    data = np.squeeze(nib.load(image_path).get_fdata())\n",
    "    data = data / data.mean()\n",
    "    data = data.reshape((1, 1) + data.shape)\n",
    "    input_tensor = torch.tensor(data, dtype=torch.float32).cuda()\n",
    "\n",
    "    # Load label\n",
    "    label = df.loc[subject_id, 'age']\n",
    "    y, bc = dpu.num2vect(np.array([label]), bin_range, bin_step, sigma)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        x = output[0].cpu().reshape([1, -1])\n",
    "        loss = dpl.my_KLDivLoss(x, y_tensor).item()\n",
    "\n",
    "    probs = np.exp(x.numpy().reshape(-1))\n",
    "    probs /= probs.sum()\n",
    "    top_idx = np.argmax(probs)\n",
    "    top_bin = float(bc[top_idx])\n",
    "    top_prob = float(probs[top_idx])\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'label': float(label),\n",
    "        'loss': loss,\n",
    "        'top_bin': top_bin,\n",
    "        'top_prob': top_prob\n",
    "    })\n",
    "\n",
    "    total_loss += loss\n",
    "    total_subjects += 1\n",
    "\n",
    "# -----------------------\n",
    "# Save results\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\n Results saved to {output_csv}\")\n",
    "\n",
    "# -----------------------\n",
    "# Overall stats\n",
    "# -----------------------\n",
    "avg_loss = total_loss / total_subjects if total_subjects > 0 else 0\n",
    "print(f\"\\n Evaluated {total_subjects} subjects\")\n",
    "print(f\"Average KLDiv loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.99974395939085\n",
      "1.776901319796959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([251., 374., 158.,  88.,  57.,  25.,  19.,   6.,   4.,   3.]),\n",
       " array([ 6.47 , 11.623, 16.776, 21.929, 27.082, 32.235, 37.388, 42.541,\n",
       "        47.694, 52.847, 58.   ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../ABIDE_Dataset/data/ABIDEI/participants.tsv', sep=\"\\t\", dtype=str)\n",
    "print(sum(df[\"age\"].astype(float))/len(df))\n",
    "\n",
    "df = pd.read_csv('../ABIDE_Dataset/outputs/ABIDEI/sfcn_age_predictions.csv')\n",
    "print((sum(df[\"label\"].astype(float)) - sum(df[\"top_bin\"].astype(float)))/len(df))\n",
    "plt.hist(df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune for ASD vs Non-ASD Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (brain_model_env)",
   "language": "python",
   "name": "brain_model_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
