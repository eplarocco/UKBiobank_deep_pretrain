{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Virtual Environment and Install Requirements\n",
    "#!python3 -m venv ../brain_model_env\n",
    "#!source ../brain_model_env/bin/activate\n",
    "#!python3 -m ipykernel install --user --name=brain_model_env --display-name \"Python (brain_model_env)\"\n",
    "#remember to switch to notebook/virtual environment kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_model.model_files.sfcn import SFCN\n",
    "from dp_model import dp_loss as dpl\n",
    "from dp_model import dp_utils as dpu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD Prediction on ABIDEI Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 985/985 [02:16<00:00,  7.20it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.4264705882352941\n",
      "ROC_AUC-score: 0.5367575462512171\n",
      "\n",
      "Finished inference on 183 subjects\n",
      "Accuracy: 57.38%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "input_root = '../ABIDE_Dataset/data/JustBrain/ABIDEI'\n",
    "participants_path = './ABIDEI/participants.tsv'\n",
    "model_weights_path = './ABIDEI/finetuned_sfcn_best.pth'\n",
    "label_column = 'label'\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN(output_dim=2, channel_number=[28, 58, 128, 256, 256, 64])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True, map_location=torch.device('cpu')))\n",
    "#model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load labels\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df = df[df.dataset == 'test'] #only test set\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference loop\n",
    "# -----------------------\n",
    "records = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through subject subfolders\n",
    "for subject_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    # Full path to T1w file\n",
    "    anat_dir = os.path.join(input_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    # Find the T1w NIfTI file (assuming one per subject)\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii.gz')]\n",
    "    t1w_file = None\n",
    "    for f in nii_files:\n",
    "        if subject_id in f and 'T1w' in f:\n",
    "            t1w_file = f\n",
    "            break\n",
    "\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(anat_dir, t1w_file)\n",
    "\n",
    "    if subject_id not in df.index: # not in test set\n",
    "        #print(f\"{subject_id} not in participants.tsv\") \n",
    "        continue\n",
    "\n",
    "    true_label = df.loc[subject_id, label_column]\n",
    "\n",
    "    # Load and normalize\n",
    "    data = nib.load(full_path).get_fdata()\n",
    "    data = data / data.mean()\n",
    "    data = dpu.crop_center(data, (160, 192, 160))\n",
    "\n",
    "    # Prepare input tensor (1, 1, D, H, W)\n",
    "    input_tensor = torch.tensor(data.reshape((1,1) + data.shape), dtype=torch.float32)#.cuda()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = output[0].cpu().reshape([1, -1])\n",
    "        probs = np.exp(probs.numpy().reshape(-1))\n",
    "\n",
    "        \n",
    "    pred_label = np.argmax(probs)\n",
    "    is_correct = int(pred_label == true_label)\n",
    "\n",
    "    correct += is_correct\n",
    "    total += 1\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'prob_nonASD': probs[0],\n",
    "        'prob_ASD': probs[1],\n",
    "        'correct': is_correct\n",
    "    })\n",
    "\n",
    "# -----------------------\n",
    "# Save and report\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv('./ABIDEI/sfcn_asd_predictions.csv', index=False)\n",
    "\n",
    "#F1 and AUC_ROC\n",
    "print(f'F1-score: {f1_score(results_df.true_label, results_df.predicted_label)}')\n",
    "print(f'ROC_AUC-score: {roc_auc_score(results_df.true_label, results_df.prob_ASD)}')\n",
    "\n",
    "\n",
    "#Accuracy\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nFinished inference on {total} subjects\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD Prediction on ABIDEII OOD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 310/961 [02:29<05:21,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-29057 not in participants.tsv\n",
      "sub-29058 not in participants.tsv\n",
      "sub-29059 not in participants.tsv\n",
      "sub-29060 not in participants.tsv\n",
      "sub-29062 not in participants.tsv\n",
      "sub-29063 not in participants.tsv\n",
      "sub-29064 not in participants.tsv\n",
      "sub-29065 not in participants.tsv\n",
      "sub-29066 not in participants.tsv\n",
      "sub-29067 not in participants.tsv\n",
      "sub-29068 not in participants.tsv\n",
      "sub-29069 not in participants.tsv\n",
      "sub-29070 not in participants.tsv\n",
      "sub-29071 not in participants.tsv\n",
      "sub-29072 not in participants.tsv\n",
      "sub-29073 not in participants.tsv\n",
      "sub-29074 not in participants.tsv\n",
      "sub-29075 not in participants.tsv\n",
      "sub-29076 not in participants.tsv\n",
      "sub-29077 not in participants.tsv\n",
      "sub-29078 not in participants.tsv\n",
      "sub-29079 not in participants.tsv\n",
      "sub-29080 not in participants.tsv\n",
      "sub-29081 not in participants.tsv\n",
      "sub-29082 not in participants.tsv\n",
      "sub-29083 not in participants.tsv\n",
      "sub-29085 not in participants.tsv\n",
      "sub-29086 not in participants.tsv\n",
      "sub-29087 not in participants.tsv\n",
      "sub-29088 not in participants.tsv\n",
      "sub-29089 not in participants.tsv\n",
      "sub-29090 not in participants.tsv\n",
      "sub-29091 not in participants.tsv\n",
      "sub-29092 not in participants.tsv\n",
      "sub-29093 not in participants.tsv\n",
      "sub-29094 not in participants.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 961/961 [07:52<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.3916913946587537\n",
      "ROC_AUC-score: 0.5682258732309545\n",
      "\n",
      "Finished inference on 924 subjects\n",
      "Accuracy: 55.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "input_root = '../ABIDE_Dataset/data/JustBrain/ABIDEII'\n",
    "participants_path = './ABIDEII/participants.tsv'\n",
    "model_weights_path = './ABIDEI/finetuned_sfcn_best.pth'\n",
    "label_column = 'label'\n",
    "\n",
    "# -----------------------\n",
    "# Load model\n",
    "# -----------------------\n",
    "model = SFCN(output_dim=2, channel_number=[28, 58, 128, 256, 256, 64])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_weights_path, weights_only=True, map_location=torch.device('cpu')))\n",
    "#model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load labels\n",
    "# -----------------------\n",
    "df = pd.read_csv(participants_path, sep='\\t')\n",
    "df['participant_id'] = df['participant_id'].str.strip()\n",
    "df = df.set_index('participant_id')\n",
    "\n",
    "# -----------------------\n",
    "# Inference loop\n",
    "# -----------------------\n",
    "records = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Loop through subject subfolders\n",
    "for subject_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    if not subject_id.startswith('sub-'):\n",
    "        continue\n",
    "\n",
    "    # Full path to T1w file\n",
    "    anat_dir = os.path.join(input_root, subject_id, 'anat')\n",
    "    if not os.path.isdir(anat_dir):\n",
    "        continue\n",
    "\n",
    "    # Find the T1w NIfTI file (assuming one per subject)\n",
    "    nii_files = [f for f in os.listdir(anat_dir) if f.endswith('.nii.gz')]\n",
    "    t1w_file = None\n",
    "    for f in nii_files:\n",
    "        if subject_id in f and 'T1w' in f:\n",
    "            t1w_file = f\n",
    "            break\n",
    "\n",
    "    if t1w_file is None:\n",
    "        print(f\"No T1w file found for {subject_id}\")\n",
    "        continue\n",
    "\n",
    "    full_path = os.path.join(anat_dir, t1w_file)\n",
    "\n",
    "    if subject_id not in df.index:\n",
    "        print(f\"{subject_id} not in participants.tsv\")\n",
    "        continue\n",
    "\n",
    "    true_label = df.loc[subject_id, label_column]\n",
    "\n",
    "    # Load and normalize\n",
    "    data = nib.load(full_path).get_fdata()\n",
    "    data = data / data.mean()\n",
    "    data = dpu.crop_center(data, (160, 192, 160))\n",
    "\n",
    "    # Prepare input tensor (1, 1, D, H, W)\n",
    "    input_tensor = torch.tensor(data.reshape((1,1) + data.shape), dtype=torch.float32)#.cuda()\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = output[0].cpu().reshape([1, -1])\n",
    "        probs = np.exp(probs.numpy().reshape(-1))\n",
    "\n",
    "        \n",
    "    pred_label = np.argmax(probs)\n",
    "    is_correct = int(pred_label == true_label)\n",
    "\n",
    "    correct += is_correct\n",
    "    total += 1\n",
    "\n",
    "    records.append({\n",
    "        'subject_id': subject_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'prob_nonASD': probs[0],\n",
    "        'prob_ASD': probs[1],\n",
    "        'correct': is_correct\n",
    "    })\n",
    "\n",
    "# -----------------------\n",
    "# Save and report\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(records)\n",
    "results_df.to_csv('./ABIDEII/sfcn_asd_predictions.csv', index=False)\n",
    "\n",
    "#F1 and AUC_ROC\n",
    "print(f'F1-score: {f1_score(results_df.true_label, results_df.predicted_label)}')\n",
    "print(f'ROC_AUC-score: {roc_auc_score(results_df.true_label, results_df.prob_ASD)}')\n",
    "\n",
    "\n",
    "#Accuracy\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nFinished inference on {total} subjects\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (brain_model_env)",
   "language": "python",
   "name": "brain_model_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
